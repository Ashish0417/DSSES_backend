# -*- coding: utf-8 -*-
"""Week1_DSSE_TXT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6mhkIccGQOznSGwqKUH81ca7AbV75ke
"""

import os
import base64
from Crypto.Cipher import DES
from Crypto.Random import get_random_bytes

def generate_key():
    return get_random_bytes(8)

key = generate_key()
# print("Generated DES Key:", key)

key = "b'\xa9\x89\xa4\x13\x94\xf6\xe7H'"

from Crypto.Cipher import DES
from Crypto.Util.Padding import pad, unpad

def des_encrypt(plain_text, key):
    cipher = DES.new(key, DES.MODE_ECB)
    padded_text = pad(plain_text.encode('utf-8'), DES.block_size)  # PKCS7 padding
    encrypted_text = cipher.encrypt(padded_text)
    return base64.b64encode(encrypted_text).decode()

def des_decrypt(encrypted_text, key):
    cipher = DES.new(key, DES.MODE_ECB)
    decrypted_padded_text = cipher.decrypt(base64.b64decode(encrypted_text))
    decrypted_text = unpad(decrypted_padded_text, DES.block_size).decode('utf-8')  # Proper decoding
    return decrypted_text




def tokenize_document(content):
    return set(content.split())


def encrypt_keywords(words, key):
    return {des_encrypt(word, key) for word in words}



def build_index(documents, key):
    index = {}
    for doc_id, content in enumerate(documents):
        words = tokenize_document(content)
        encrypted_words = encrypt_keywords(words, key)
        for e_word in encrypted_words:
            if e_word not in index:
                index[e_word] = []
            index[e_word].append(doc_id)
    return index



def generate_search_token(keyword, key):
    return des_encrypt(keyword, key)



def search(keyword, key, index, encrypted_docs):
    encrypted_keyword = generate_search_token(keyword, key)
    if encrypted_keyword in index:
        print(f"Keyword found in documents: {index[encrypted_keyword]}")
        for doc_id in index[encrypted_keyword]:
            print(f"Decrypted Document {doc_id}: {des_decrypt(encrypted_docs[doc_id], key)}")
    else:
        print("No matches found.")


def multi_keyword_search(keywords, key, index, encrypted_docs):
    encrypted_keywords = {generate_search_token(word, key) for word in keywords}
    matching_docs = set()

    for enc_word in encrypted_keywords:
        if enc_word in index:
            if not matching_docs:
                matching_docs = set(index[enc_word])
            else:
                matching_docs.intersection_update(set(index[enc_word]))

    if matching_docs:
        print(f"Matching Documents: {matching_docs}")
        for doc_id in matching_docs:
            print(f"Decrypted Document {doc_id}: {des_decrypt(encrypted_docs[doc_id], key)}")
    else:
        print("No matches found.")


def add_document(new_doc, key, index, encrypted_docs):
    doc_id = len(encrypted_docs)
    encrypted_doc = des_encrypt(new_doc, key)
    encrypted_docs.append(encrypted_doc)

    words = tokenize_document(new_doc)
    encrypted_words = encrypt_keywords(words, key)

    for e_word in encrypted_words:
        if e_word not in index:
            index[e_word] = []
        index[e_word].append(doc_id)

    print(f"Document {doc_id} added successfully.")

    return encrypted_doc



def delete_document(doc_id, index, encrypted_docs):
    encrypted_docs[doc_id] = None  # Mark document as deleted
    for enc_word in index.keys():
        if doc_id in index[enc_word]:
            index[enc_word].remove(doc_id)

    print(f"Document {doc_id} deleted successfully.")


def read_document(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()  # Read entire content as a string



def save_encrypted_document(file_path, encrypted_text):
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(encrypted_text)



def build_index(document, key):
    words = set(document.split())  # Unique words
    index = {des_encrypt(word, key): [] for word in words}  # Encrypt words
    for word in words:
        index[des_encrypt(word, key)].append(word)  # Store plaintext mapping
    return index

def encrypt_and_index_document(file_path, key):
    document_text = read_document(file_path)
    encrypted_doc = add_document(document_text,key,index,encrypted_docs)
    encrypted_file_path = f"/content/encrypted_{os.path.basename(file_path)}"
    save_encrypted_document(encrypted_file_path, encrypted_doc)
    encrypted_index = index
    return encrypted_file_path, encrypted_index



def read_encrypted_document(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

def save_decrypted_document(file_path, decrypted_text):
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(decrypted_text)


def decrypt_and_store(file_path, key):
    encrypted_text = read_encrypted_document(file_path)
    decrypted_text = des_decrypt(encrypted_text, key)
    decrypted_file_path = f"/content/decrypted_{os.path.basename(file_path)}"
    save_decrypted_document(decrypted_file_path, decrypted_text)

    return decrypted_file_path



from difflib import SequenceMatcher


def similarity_percentage(original, decrypted):
    """Calculate similarity percentage between original and decrypted text"""
    return SequenceMatcher(None, original, decrypted).ratio() * 100

# original_text = read_document("/content/asmita.txt")
# decrypted_text = read_document("/content/decrypted_encrypted_asmita.txt")

# similarity = similarity_percentage(original_text, decrypted_text)
# print(f"Original Text: {original_text}")
# print(f"Decrypted Text: {decrypted_text}")
# print(f"Similarity Percentage: {similarity:.2f}%")

